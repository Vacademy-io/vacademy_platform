spring.application.name=media_service
server.port=8075
management.endpoints.web.base-path=/media-service
management.endpoints.web.path-mapping.health=actuator/health
springdoc.api-docs.path=/media-service/api-docs
springdoc.swagger-ui.path=/media-service/swagger-ui.html
spring.profiles.active=${environment}
spring.ai.openai.api-key=${DEEPSEEK_API_KEY}
spring.ai.openai.base-url=https://api.deepseek.com
spring.ai.openai.chat.options.model=deepseek-chat
# Keep temperature low for structured data generation
spring.ai.openai.chat.options.temperature=0.6
spring.ai.openai.chat.options.responseFormat.type=json_object
spring.ai.retry.max-attempts=3
spring.ai.retry.initial-interval=1s
spring.ai.retry.max-interval=5s
# Increase max tokens for large documents
spring.ai.openai.chat.options.maxTokens=8192
# The DeepSeek API doesn't support embeddings, so we need to disable it.
spring.ai.openai.embedding.enabled=false
spring.servlet.multipart.max-file-size=10MB
spring.servlet.multipart.max-request-size=10MB
spring.jackson.deserialization.fail_on_unknown_properties=false



# ============ HikariCP Connection Pool Configuration ============
# These settings are CRITICAL for preventing inconsistent API latency

# Maximum number of connections in the pool
spring.datasource.hikari.maximum-pool-size=5

# Minimum number of idle connections to maintain
spring.datasource.hikari.minimum-idle=1

# How long (ms) to wait for a connection from the pool
spring.datasource.hikari.connection-timeout=60000

# How long a connection can be idle before being retired
spring.datasource.hikari.idle-timeout=300000

# Maximum lifetime of a connection
spring.datasource.hikari.max-lifetime=600000

# Connection validation timeout
spring.datasource.hikari.validation-timeout=5000

# Keep connections alive
spring.datasource.hikari.keepalive-time=60000

# Log pool usage warnings
spring.datasource.hikari.leak-detection-threshold=60000

# Sentry Configuration
sentry.dsn=${SENTRY_DSN}
sentry.send-default-pii=true

# Performance Monitoring - 100% of transactions (reduce in production for cost optimization)
sentry.traces-sample-rate=1.0
# Enable performance monitoring for all integrations
sentry.enable-tracing=true
# Database query monitoring
sentry.trace-propagation-targets=localhost,127.0.0.1
# Additional performance settings
sentry.in-app-includes=vacademy.io

# ==================== AI Model Configuration ====================
# Default AI model to use when none specified
ai.default-model=google/gemini-2.5-flash

# List of allowed models that frontend can request
ai.allowed-models=google/gemini-2.5-flash,google/gemini-2.0-flash,openai/gpt-4o-mini,openai/gpt-4o,anthropic/claude-3-haiku,anthropic/claude-3-sonnet,deepseek/deepseek-chat,mistralai/mistral-large

# Fallback models to try when primary model fails
ai.fallback-models=google/gemini-2.5-flash,openai/gpt-4o-mini,deepseek/deepseek-chat

# Maximum retry attempts for AI calls
ai.max-retry-attempts=5

# Default timeout in milliseconds for AI API calls
ai.default-timeout-ms=30000

# PDF processing configuration
ai.pdf.max-tries=20
ai.pdf.delay-ms=20000

# Audio processing configuration
ai.audio.max-tries=50
ai.audio.delay-ms=20000

# ==================== AI Service URL (for token usage logging) ====================
# Uses AI_SERVICE_BASE_URL from ConfigMap in K8s, falls back to localhost for local dev
ai.service.url=${AI_SERVICE_BASE_URL:http://localhost:8000}
